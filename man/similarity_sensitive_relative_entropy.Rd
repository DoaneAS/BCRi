% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/relative_diversity.R
\name{similarity_sensitive_relative_entropy}
\alias{similarity_sensitive_relative_entropy}
\title{Calculate Similarity-Sensitive Relative Entropy and Cross-Entropy for Multiple Distributions}
\usage{
similarity_sensitive_relative_entropy(
  probability_matrix,
  similarity_matrix,
  reference_distribution = NULL,
  base = exp(1),
  normalize_similarity = FALSE,
  symmetric = FALSE,
  return_cross_entropy = TRUE,
  distribution_names = NULL
)
}
\arguments{
\item{probability_matrix}{A matrix where each column represents a probability distribution.
Each column should sum to 1.}

\item{similarity_matrix}{A square matrix of similarities between categories.
Dimensions should match the number of rows in the probability_matrix.}

\item{reference_distribution}{The column index of the distribution to use as the reference (Q).
If NULL, all pairwise entropy measures are calculated.}

\item{base}{The logarithm base to use (default: natural logarithm, base e)}

\item{normalize_similarity}{Logical. Whether to normalize the similarity matrix
so that rows sum to 1. Default is FALSE.}

\item{symmetric}{Logical. If TRUE, calculate a symmetric version of the divergence.
Default is FALSE (standard asymmetric KL divergence).}

\item{return_cross_entropy}{Logical. Whether to return cross-entropy values along with
relative entropy. Default is TRUE.}

\item{distribution_names}{Optional character vector of names for the distributions.}
}
\value{
A list containing:
- relative_entropy: Matrix or vector of relative entropy values
- cross_entropy: (if return_cross_entropy=TRUE) Matrix or vector of cross-entropy values
}
\description{
This function computes similarity-sensitive relative entropy (Kullback-Leibler divergence)
and cross-entropy between multiple probability distributions organized as column vectors.
}
\details{
The similarity-sensitive relative entropy between distributions P and Q is defined as:
\deqn{D_{KL,S}(P||Q) = \sum_{i=1}^{n} \sum_{j=1}^{n} s_{ij} p_i \log(p_j/q_j)}
The similarity-sensitive cross-entropy is:
\deqn{H_S(P,Q) = -\sum_{i=1}^{n} \sum_{j=1}^{n} s_{ij} p_i \log(q_j)}
}
\examples{
# Create probability distributions as column vectors
probs_matrix <- cbind(
  c(0.5, 0.3, 0.2),  # Distribution 1
  c(0.2, 0.3, 0.5),  # Distribution 2
  c(1/3, 1/3, 1/3)   # Distribution 3 (uniform)
)

# Create similarity matrix
sim_matrix <- matrix(c(
  1.0, 0.3, 0.1,
  0.3, 1.0, 0.6,
  0.1, 0.6, 1.0
), nrow = 3)

# Calculate relative entropy and cross-entropy
result <- similarity_sensitive_relative_entropy(probs_matrix, sim_matrix)
print(result)

}
